{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85e2eaf",
   "metadata": {},
   "source": [
    "This will be the main notebook, where data is loaded, investigated, prepared, and models are build, trained and experimented with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2d8108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb9adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_IMAGES = os.path.join('Kvasir-SEG', 'images')\n",
    "PATH_LABELS = os.path.join('Kvasir-SEG', 'bbox')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4b8b7",
   "metadata": {},
   "source": [
    "## Data Analysing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "433fda73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_images(directory_path):\n",
    "    \"\"\" Returns a list of file paths to all .jpg images in the given directory. \"\"\"\n",
    "    image_files = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith('.jpg'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            image_files.append(file_path)\n",
    "    return sorted(image_files)\n",
    "\n",
    "image_paths = load_images(PATH_IMAGES)\n",
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79330ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1071, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_labels(label_dir):\n",
    "    \"\"\" \n",
    "    Loads all CSV label files from the specified directory. \n",
    "    Open each file, and save corners of each bounding box together with id/filename as new row.\n",
    "    Combine everything into a single DataFrame and return it.\n",
    "    \"\"\"\n",
    "    all_labels = []\n",
    "\n",
    "    for filename in sorted(os.listdir(label_dir)):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_id = os.path.splitext(filename)[0]\n",
    "            df = pd.read_csv(os.path.join(label_dir, filename))\n",
    "            df[\"id\"] = file_id\n",
    "            all_labels.append(df)\n",
    "\n",
    "    labels_df = pd.concat(all_labels, ignore_index=True)\n",
    "\n",
    "    return labels_df\n",
    "\n",
    "labels_df = load_labels(PATH_LABELS)\n",
    "labels_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b191e5",
   "metadata": {},
   "source": [
    "Checking, if some images have multiple bounding boxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce86c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "cju3uhb79gcgr0871orbrbi3x    10\n",
       "cju414lf2l1lt0801rl3hjllj     4\n",
       "cju32a52lb9rc0799xi40qs00     3\n",
       "cju0roawvklrq0799vmjorwfv     3\n",
       "cju43c92lm5cj0755lorsorfg     3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df['id'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a136ef52",
   "metadata": {},
   "source": [
    "Checking if some images have no bounding box at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65aaeeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids = {os.path.splitext(os.path.basename(path))[0] for path in image_paths}\n",
    "labeled_ids = set(labels_df['id'].unique())\n",
    "unlabeled_ids = image_ids - labeled_ids\n",
    "len(unlabeled_ids)  # should be 0 if all images are labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26558fc",
   "metadata": {},
   "source": [
    "## Dynamic Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c734756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KvasirPolypYOLODataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Kvasir-SEG in YOLO format.\n",
    "\n",
    "    __getitem__ returns:\n",
    "        - image: Tensor (3, H, W) float32 in [0, 1]\n",
    "        - targets: Tensor (N, 5) with [class, x_center, y_center, w, h] normalized\n",
    "                  (N = number of bboxes for that image, can be 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir,\n",
    "        labels_df,\n",
    "        img_size=None,      # optional resizing to (img_size, img_size), 640 is common for YOLO\n",
    "        default_class=0,    \n",
    "        transforms=None     # optional add transform\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.images_dir = images_dir\n",
    "        self.img_paths = load_images(images_dir)\n",
    "        self.img_size = img_size\n",
    "        self.default_class = default_class\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # pre-build a dictionary id -> dataframe rows (bboxes)\n",
    "        # id is the filename without extension (like \"image_001\")\n",
    "        self.id_to_boxes = {}\n",
    "        for img_id, group in labels_df.groupby(\"id\"):\n",
    "            self.id_to_boxes[img_id] = group.reset_index(drop=True)\n",
    "\n",
    "        # ensure no images without labels\n",
    "        image_ids = {os.path.splitext(os.path.basename(p))[0] for p in self.img_paths}\n",
    "        labeled_ids = set(self.id_to_boxes.keys())\n",
    "        unlabeled_ids = image_ids - labeled_ids\n",
    "        print(f\"Total images: {len(image_ids)}\")\n",
    "        print(f\"Images with at least one bbox: {len(labeled_ids)}\")\n",
    "        print(f\"Images without bbox: {len(unlabeled_ids)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image\n",
    "        img_path = self.img_paths[idx]\n",
    "        img_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "        # read image with cv2 (BGR)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            raise RuntimeError(f\"Cannot read image: {img_path}\")\n",
    "\n",
    "        # BGR -> RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        orig_h, orig_w = img.shape[:2]\n",
    "\n",
    "        # optional resizing (for YOLO often 640x640 or similar)\n",
    "        if self.img_size is not None:\n",
    "            img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "            new_h, new_w = self.img_size, self.img_size\n",
    "        else:\n",
    "            new_h, new_w = orig_h, orig_w\n",
    "\n",
    "        # normalize to [0,1] and change to (C,H,W)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = np.transpose(img, (2, 0, 1))  # (H,W,C) -> (C,H,W)\n",
    "        img_tensor = torch.from_numpy(img)  # img tensor\n",
    "\n",
    "        # all bboxes for this image, None if no bboxes\n",
    "        df_boxes = self.id_to_boxes.get(img_id, None)\n",
    "\n",
    "        targets = []\n",
    "\n",
    "        if df_boxes is not None and len(df_boxes) > 0:\n",
    "            for _, row in df_boxes.iterrows():\n",
    "                xmin = float(row[\"xmin\"])\n",
    "                ymin = float(row[\"ymin\"])\n",
    "                xmax = float(row[\"xmax\"])\n",
    "                ymax = float(row[\"ymax\"])\n",
    "\n",
    "                # if resized, scale the coordinates\n",
    "                if self.img_size is not None:\n",
    "                    # scaling factor\n",
    "                    scale_x = new_w / orig_w\n",
    "                    scale_y = new_h / orig_h\n",
    "                    xmin *= scale_x\n",
    "                    xmax *= scale_x\n",
    "                    ymin *= scale_y\n",
    "                    ymax *= scale_y\n",
    "\n",
    "                # convert to normalized YOLO format\n",
    "                box_w = xmax - xmin\n",
    "                box_h = ymax - ymin\n",
    "                x_center = xmin + box_w / 2.0\n",
    "                y_center = ymin + box_h / 2.0\n",
    "\n",
    "                x_center /= new_w\n",
    "                y_center /= new_h\n",
    "                box_w    /= new_w\n",
    "                box_h    /= new_h\n",
    "\n",
    "                # check for valid boxes\n",
    "                if box_w <= 0 or box_h <= 0:\n",
    "                    continue\n",
    "\n",
    "                # [class, x_center, y_center, w, h]\n",
    "                targets.append([\n",
    "                    float(self.default_class),\n",
    "                    float(x_center),\n",
    "                    float(y_center),\n",
    "                    float(box_w),\n",
    "                    float(box_h),\n",
    "                ])\n",
    "\n",
    "        if len(targets) > 0:\n",
    "            targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        else:\n",
    "            # in case of no bboxes put zero-tensor (0,5)\n",
    "            targets = torch.zeros((0, 5), dtype=torch.float32)\n",
    "\n",
    "        # optional: implement transformations here (on img and bbox)\n",
    "\n",
    "        return img_tensor, targets, img_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d0c5746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for YOLO-format batches.\n",
    "    \n",
    "    Takes a list of (image, targets, img_id) tuples and combines them into:\n",
    "        - images: stacked tensor of shape (B, 3, H, W)\n",
    "        - all_targets: concatenated tensor of shape (M, 6) where each row is \n",
    "          [batch_index, class, x_center, y_center, width, height]\n",
    "        - img_ids: list of image identifiers\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    all_targets = []\n",
    "    img_ids = []\n",
    "\n",
    "    for i, (img, targets, img_id) in enumerate(batch):\n",
    "        images.append(img)\n",
    "        img_ids.append(img_id)\n",
    "\n",
    "        if targets.numel() > 0:\n",
    "            # add column with the index of the image in the batch\n",
    "            batch_idx = torch.full(\n",
    "                (targets.size(0), 1),\n",
    "                i,\n",
    "                dtype=targets.dtype\n",
    "            )\n",
    "            # [batch_idx, class, x_c, y_c, w, h]\n",
    "            t = torch.cat([batch_idx, targets], dim=1)\n",
    "            all_targets.append(t)\n",
    "\n",
    "    images = torch.stack(images, dim=0)  # (B,3,H,W)\n",
    "\n",
    "    if len(all_targets) > 0:\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "    else:\n",
    "        all_targets = torch.zeros((0, 6), dtype=torch.float32)\n",
    "\n",
    "    return images, all_targets, img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d9170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1000\n",
      "Images with at least one bbox: 1000\n",
      "Images without bbox: 0\n"
     ]
    }
   ],
   "source": [
    "dataset = KvasirPolypYOLODataset(\n",
    "    images_dir=PATH_IMAGES,\n",
    "    labels_df=labels_df,\n",
    "    img_size=640,   \n",
    "    default_class=0,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=2,  \n",
    "    collate_fn=yolo_collate_fn\n",
    ")\n",
    "\n",
    "# prova a iterare un batch\n",
    "for images, targets, img_ids in dataloader:\n",
    "    print(\"Batch images shape:\", images.shape)  # (B,3,H,W)\n",
    "    print(\"Targets shape:\", targets.shape)      # (M,6) -> [batch_idx, class, x_c, y_c, w, h]\n",
    "    print(\"Example img_ids:\", img_ids[:3])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polyp-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
